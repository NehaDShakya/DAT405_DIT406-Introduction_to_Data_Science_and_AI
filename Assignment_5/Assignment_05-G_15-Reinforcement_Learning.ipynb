{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 5: Reinforcement learning\n",
        "\n",
        "**Submitted by:** *Neha Devi Shakya 15h, Sarvesh Meenowa 15h*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpPo0Q36Cd5s"
      },
      "source": [
        "General guidelines:\n",
        "*   All solutions to theoretical and pratical problems must be submitted in this ipynb notebook, and equations wherever required, should be formatted using LaTeX math-mode.\n",
        "*   All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.\n",
        "*   Your name, personal number and email address should be specified above.\n",
        "*   All tables and other additional information should be included in this notebook.\n",
        "*   Before submitting, make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx1slCKECd5z"
      },
      "source": [
        "Self-check \n",
        "1. Have you answered all questions to the best of your ability? \n",
        "2. Anything else you can easily check? (details, terminology, arguments, commenting for code etc.?) \n",
        "\n",
        "Grading will be based on a qualitative assessment of each assignment. It is important to:\n",
        "*\tPresent clear arguments\n",
        "*\tPresent the results in a pedagogical way\n",
        "*\tShow understanding of the topics (e.g, write a pseudocode) \n",
        "*\tGive correct solutions\n",
        "*\tMake sure that the code is well commented \n",
        "\n",
        "**Again, as mentioned in general guidelines, all code should be written here. And this same ipython notebook file (RLAssignment.ipynb) should be submitted with answers and code written in it. Ipython notebook is mandatory submission. And also submit HTML version of it for easy readibility (goto File/Download as). NO OTHER FORMAT SHALL BE ACCEPTED.** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xkuSQXzCd50"
      },
      "source": [
        "# Primer\n",
        "\n",
        "## Decision Making\n",
        "The problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\n",
        "two parts. First, how do we learn about the world? This involves both the\n",
        "problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\n",
        "currently know about the world, how should we decide what to do, taking into\n",
        "account future events and observations that may change our conclusions?\n",
        "Typically, this will involve creating long-term plans covering possible future\n",
        "eventualities. That is, when planning under uncertainty, we also need to take\n",
        "into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\n",
        "things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\n",
        "known to produce good results and experiment with something new is known\n",
        "as the **exploration-exploitation dilemma**.\n",
        "\n",
        "## The exploration-exploitation trade-off\n",
        "\n",
        "Consider the problem of selecting a restaurant to go to during a vacation.Lets say the\n",
        "best restaurant you have found so far was **Les Epinards**. The food there is\n",
        "usually to your taste and satisfactory. However, a well-known recommendations\n",
        "website suggests that **King’s Arm** is really good! It is tempting to try it out. But\n",
        "there is a risk involved. It may turn out to be much worse than **Les Epinards**,\n",
        "in which case you will regret going there. On the other hand, it could also be\n",
        "much better. What should you do?\n",
        "It all depends on how much information you have about either restaurant,\n",
        "and how many more days you’ll stay in town. If this is your last day, then it’s\n",
        "probably a better idea to go to **Les Epinards**, unless you are expecting **King’s\n",
        "Arm** to be significantly better. However, if you are going to stay there longer,\n",
        "trying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\n",
        "better food for the remaining time, while otherwise you will have missed only\n",
        "one good meal out of many, making the potential risk quite small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox6E2goECd51"
      },
      "source": [
        "## Overview\n",
        "* To make things concrete, we will first focus on decision making under **no** uncertainity, i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We shall first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n",
        "\n",
        "\n",
        "* Next, we will work through one type of reinforcement learning algorithm called Q-learning. Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRV5u3pGCd52"
      },
      "source": [
        "## Markov Decision Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A75eKq-iCd53"
      },
      "source": [
        "Markov Decision Process (MDP) provides a mathematical framework for modeling decision-making. It is a discrete time (distinct points in time) stochastic (randomly determined) process.\n",
        "\n",
        "MDPs are made up of 4 parts:  \n",
        "S: Finite set of states (Ex: s<sub>1</sub>, s<sub>2</sub> ... s<sub>N</sub>)  \n",
        "A: Finite set of actions (Ex: North, South, East, West)  \n",
        "P<sub>a</sub>(s,s'): Probability that action *a* in state *s* at time *t* will lead to state *s'* at time *t + 1*  \n",
        "R<sub>a</sub>(s,s'): Immediate reward received after moving from state *s* to state *s'* by action *a*\n",
        "\n",
        "An agent acts in an MDP at time *t*, by taking certain action *a* in state *s*, going to state *s'*, and getting a reward *r* from the world. It then repeats the process for certain no. of times, either finite or infinite.\n",
        "\n",
        "We also include a $5^{th}$ part in the description of an MDP called Gamma $\\gamma$.  \n",
        "$\\gamma$: The discount factor between 0 (inclusive) and 1 (exclusive). This determines how much credit you want to give to the future. If you think that the future reward is as important as the current reward you would set this to 0.99999. If you don't care about the future rewards you would set this to 0 and you only care about the current reward. For example, if your discount factor is 0.8 and after 5 steps you get a reward of 4 the present value of that reward is $0.8^4 * 5$ or ~2.\n",
        "\n",
        "An MDP is a collection of states such that each state has a selection of actions associated with them. With each state-action pair comes a reward *r* (can be 0). Define a policy function: $\\pi: s \\rightarrow a$, which tells which action to take at each state.\n",
        "  \n",
        "We now use the famous dynamic programming equation, also known as Bellman Equation, to define optimality in an MDP. The following equation defines what we call the **value function** of state *s* following some fixed policy $\\pi$:  \n",
        "\n",
        "$$V^\\pi(s) = \\sum_{s'} P_{\\pi(s)}(s,s') [R_{\\pi(s)}(s,s') + \\gamma V^\\pi(s')]$$\n",
        "\n",
        "We call $V^\\pi$ as the value of policy $\\pi$.  \n",
        "  \n",
        "Now, to find the **optimal** policy you will need to find the action that gives the highest reward.  \n",
        "\n",
        "$$V^*(s) = max_a \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^*(s')]$$\n",
        "\n",
        "A real world example would be an inventory control system. Your states would be the amount of items you have in stock. Your actions would be the amount to order. The discrete time would be the days of the month. The reward would be the profit.  \n",
        "\n",
        "A major drawback of MDPs is called the \"Curse of Dimensionality\". This states that the more states/actions you have the more computational difficult it is to solve.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHE5j3qeCd53"
      },
      "source": [
        "## Question 1 (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMxBSRo-Cd54"
      },
      "source": [
        "For the first question of the notebook, we give a quick example of an MDP. We would to see if you can put the definitions above into practice.\n",
        "\n",
        "**Question a**: Given the following deterministic MDP (you select North, you move North), what is the optimal policy (path with the most points)?\n",
        "  \n",
        "*Notes*:  \n",
        "  * The number in the box is the reward.  \n",
        "  * Once you hit the end you are done. (Absorbing state)\n",
        "  * S is the starting point.  \n",
        "  * F is the ending point.  \n",
        "  * Use N for North, E for East, S for South, and W for West. Not all actions are available at each state, for example, you can't choose N and W at starting state, as there exists no valid next states in those directions.  \n",
        "  * Pass the directions as a single string. Ex: ESWN will make a circle.  \n",
        "  \n",
        "\n",
        "\n",
        "| | | |\n",
        "|----------|----------|---------|\n",
        "|S|1|1|\n",
        "|1 |0|1|  \n",
        "|-1|-1|0|  \n",
        "|0 |0|F|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Oxp1nnCd55"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "Assuming standard coordinate directions: North -> UP, East -> LEFT, South -> DOWN and West -> RIGHT. The optimal policy path that has the most points is <b>SENESS</b> rewarding us with <b>4 points</b>. However, one could go from the state <b>S -> (0, 0)</b> and go North to <b>(1, 0)</b>  and continue through the states <b>(1, 1)</b>, <b>(0, 1)</b>, <b>(0, 2)</b> and <b>(1, 2)</b> i.e. <b>(0 + 1 + 1 + 1)</b> and easy loop through them (last 4 mentioned states) indefinitely and ultimately go South twice to the state <b>F -> (2, 3)</b> to obtain more points.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSQCikcdCd55"
      },
      "source": [
        "Question b,c will attempt to firm up your knowledge of the parts of an MDP. Just remember that for a state denoted by (x,y), state N/E/S/W to that are (x,y-1),(x+1,y),(x,y+1),(x-1,y) respectively. We take (0,0) as the starting state S."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVwh7l6MCd56"
      },
      "source": [
        "**Question b:** What is the probability of going from state (1,0) to state (2,0) using action E ? ( i.e,  $P_E((1,0),(2,0))$ )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coYn4S_dCd56"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "For the given deterministic MDP, when we start from the state <b>(1, 0)</b>, we have a <b>100% probability</b> of reaching the state <b>(2, 0)</b> when using action <i>E</i> since <i>E</i> is <b>(x + 1, y)</b>.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG7uFAuaCd56"
      },
      "source": [
        "**Question c:** What is the reward for moving from state (1,0) to state (2,0) ? ( i.e, $R_E((1,0),(2,0))$ )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVOeAYxtCd57"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "For the given deterministic MDP, when we start from the state <b>(1, 0)</b> and go to the state <b>(2, 0)</b>, we earn a reward of <b>1 point</b> since the box of state <b>(2, 0)</b> contains the value of <b>1</b>.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iiye9PafCd57"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxAAiWs3Cd57"
      },
      "source": [
        "\n",
        "The value function is based on the Bellman Equation for optimal value, which we recall here:  \n",
        "$$V^*(s) = max_a \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^*(s')]$$\n",
        "\n",
        "The value iteration (VI) is one algorithm that can be used to find the optimal policy ($\\pi^*$). Note that for any policy $\\pi^*$ to be optimal, it must satisfy the Bellman equation for optimal value function $V^*$. For any candidate $V^*$, it must be such that plugging it in the RHS (right-hand-side) of Bellman equation should give the same $V^*$ again (by the recursive nature of this equation). This property forms the basis of VI algorithm. Essentially, due to certain mathematical results, repeated application of RHS to any intial value function $V^0(s)$ will eventually lead to the value $V^*$ which statifies the Bellman equation. Once converged*, one can extract the optimal actions by simply noting the actions that satisfy the equation.\n",
        "\n",
        "\n",
        "Note: By 'converge', we mean that the quantity of interest doesn't change anymore by further iterations of the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFwmAUmICd58"
      },
      "source": [
        "The value iteration algorithm practically procedes as follows:\n",
        "\n",
        "```\n",
        "epsilon is a small value, threshold\n",
        "V_k[s] = 0 for all s\n",
        "while |V_k[s]-V_k-1[s]| > epsilon for any s\n",
        "do\n",
        "    for each state s\n",
        "    do\n",
        "        V_k[s] = max_a Σ_s' P_a(s,s')*(R_a(s,s′) + γ*V_k−1[s′])\n",
        "    end\n",
        "end\n",
        "\n",
        "for each state s\n",
        "do\n",
        "    π(s)=argmax_a ∑_s′ P_a(s,s')*(R_a(s,s′) + γ*V_k[s′])\n",
        "end\n",
        "\n",
        "return π, V_k\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i45thql0Cd58"
      },
      "source": [
        "Example: Below is a 3x3 grid. We are going to walk through a few iterations to firm up your understanding. Lets assume this time that success of taking any action is 0.8. Meaning if we take E from a valid state (x,y), we will go (x+1,y) 0.8 percent of time, but remain in same state the remaining time. We will have a discount factor ($\\gamma$) of 0.9. Assume $V^0(s')=0$ for all s'. \n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|0|0|\n",
        "|0|10|0|  \n",
        "|0|0|0|  \n",
        "\n",
        "\n",
        "**Iteration 1**: It is trivial, V(s) becomes the $max_a \\sum_{s'} P_a(s,s') R_a(s,s')$ since $V^0$ was zero for s'.\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|8|0|\n",
        "|8|2|8|  \n",
        "|0|8|0|  \n",
        "  \n",
        "**Iteration 2**:  \n",
        "  \n",
        "Staring with cell (0,0): We find the expected value of each move:  \n",
        "Action N: 0  \n",
        "Action E: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action S: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action W: 0\n",
        "\n",
        "Hence any action between E and S would have been best at this stage.\n",
        "\n",
        "Similarly for cell (1,0):\n",
        "\n",
        "Action S: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action S is the maximizing action)  \n",
        "\n",
        "Similar calculations for remaining cells give us:\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|5.76|10.88|5.76|\n",
        "|10.88|8.12|10.88|  \n",
        "|5.76|10.88|5.76|  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-95hGNyiCd59"
      },
      "source": [
        "## Question 2 (2 points)\n",
        "Please code the value iteration algorithm just described, and show the optimal value function of the above 3x3 grid problem at convergence (the value function doesn't change anymore)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z_8HA2EvCd59"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import numpy as np\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY6y-Gw5Cd5_",
        "outputId": "86f3e3c4-8f34-4f1a-c247-cae69f42c487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pi (Index of Max Rewards):\n",
            "[(0, 1), (1, 0), (1, 2), (2, 1)]\n",
            "\n",
            "Optimal V from value iteration:\n",
            "[[45.60078618945259, 51.93591447580039, 45.60078618945259],\n",
            " [51.93591447580039, 48.03981057969649, 51.93591447580039],\n",
            " [45.60078618945259, 51.93591447580039, 45.60078618945259]]\n"
          ]
        }
      ],
      "source": [
        "def value_iteration(success_proba, gamma, reward_mtx, v_k_old, v_k, epsilon):\n",
        "    \n",
        "    # calculate new v_k till the matrices converge\n",
        "    while not np.allclose(v_k_old, v_k, atol = 0.001):\n",
        "        \n",
        "        # create copy of current v_k\n",
        "        v_k_old = v_k.copy()\n",
        "        \n",
        "        # set current v_k as empty array\n",
        "        v_k = []\n",
        "        \n",
        "        for x in range(len(v_k_old)):\n",
        "            \n",
        "            # set new row\n",
        "            row_new = []\n",
        "\n",
        "            for y in range(len(v_k_old[x])):\n",
        "                \n",
        "                # declare movements for each directions (EWNS)\n",
        "                directions = [\n",
        "                    [x + 1, y],\n",
        "                    [x - 1, y],\n",
        "                    [x, y - 1],\n",
        "                    [x, y + 1]\n",
        "                ]\n",
        "\n",
        "                # set list for possible new rewards\n",
        "                reward_new = []\n",
        "\n",
        "                for direct in directions:\n",
        "                    \n",
        "                    x_new, y_new = direct\n",
        "\n",
        "                    # check if the movement is valied and not out of bounds\n",
        "                    if int(x_new) in range(len(v_k_old)) and int(y_new) in range(len(v_k_old[0])):\n",
        "\n",
        "                        # calculate the new reward\n",
        "                        reward_new.append(success_proba * (reward_mtx[x_new][y_new] + gamma * v_k_old[x_new][y_new]) \\\n",
        "                            + (1 - success_proba) * (reward_mtx[x][y] + gamma * v_k_old[x][y]))\n",
        "\n",
        "                # add max new reward to row\n",
        "                row_new.append(max(reward_new))\n",
        "\n",
        "            # add new reward row to matrix\n",
        "            v_k.append(row_new)\n",
        "\n",
        "    # find the indices of the max rewards\n",
        "    pi = np.where(np.array(v_k) == np.array(v_k).max())\n",
        "    # return all indices\n",
        "    pi = list(zip(pi[0], pi[1]))\n",
        "\n",
        "    return pi, v_k\n",
        "\n",
        "# set the successfull action probability\n",
        "success_proba = 0.8\n",
        "\n",
        "# set the discount factor (gamma)\n",
        "gamma = 0.9\n",
        "\n",
        "# set initial reward matrix\n",
        "reward_mtx = [\n",
        "    [0,  0, 0],\n",
        "    [0, 10, 0],\n",
        "    [0,  0, 0],\n",
        "]\n",
        "\n",
        "# create an empty 3x3 matrix of zeros\n",
        "v_k = list(np.zeros((3, 3)))\n",
        "\n",
        "# create an empty 3x3 matrix of ones\n",
        "v_k_old = list(np.ones((3, 3)))\n",
        "\n",
        "# set epsilon value\n",
        "epsilon = 0.001\n",
        "\n",
        "pi, v_k = value_iteration(success_proba, gamma, reward_mtx, v_k_old, v_k, epsilon)\n",
        "\n",
        "print(f\"Pi (Index of Max Rewards):\\n{pi}\")\n",
        "print(\"\\nOptimal V from value iteration:\")\n",
        "pprint(v_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jkUTctRCd5_"
      },
      "source": [
        "## Reinforcement Learning (RL)\n",
        "Until now, we understood that knowing the MDP, specifically $P_a(s,s')$ and $R_a(s,s')$ allows us to efficiently find the optimal policy using value iteration algorithm, but RL or decision making under uncertainity arises from the question of making optimal decisions without knowing the true world model (MDP in this case).\n",
        "\n",
        "So far we have defined the value of a state $V^\\pi$ (for fixed policy). Let us now define the value of an action, $Q^\\pi$:\n",
        "\n",
        "$$Q^\\pi(s,a) = \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^\\pi(s')]$$\n",
        "\n",
        "i.e, the value of taking action *a* from state *s* and then following $\\pi$ onwards. Similarly, the optimal Q-value equation is:\n",
        "\n",
        "$$Q^*(s,a) = \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^*(s')]$$\n",
        "\n",
        "## Q-learning\n",
        "\n",
        "Q-learning algorithm can be used by an agent unaware of its surroundings (unknown MDP). All it can do is take an action *a* at time *t* from state *s* and observe the reward *r* and next state *s'*, and repeat this process again. So how it can learn to act optimally under such uninformative conditions ? Answer is using Q-learning. Without going into its justification, we simply state the main-update rule of this algorithm below:\n",
        "\n",
        "![alt text](https://chalmersuniversity.box.com/shared/static/5anbos4s9luoayb32jk6w3wy3w4jk3g3.png)\n",
        "\n",
        "Where we simply maintain Q(s,a) value for each state-action pair in a table. It is proven to converge to the optimal policy of the underlying unknown MDP for a small learning rate $\\alpha$ (you have to play around with values ranging from 0.1 to 0.00001)\n",
        "## OpenAI Gym ($\\leq 0.19.0$)\n",
        "\n",
        "We shall use already available simulators for different environments (world) using the popular OpenAI Gym library. It just implements [differnt types of simulators](https://gym.openai.com/) including ATARI games. Although here we will only focus on simple ones, such as [Chain enviroment](https://gym.openai.com/envs/NChain-v0/).\n",
        "![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\n",
        "\n",
        "## Question 3 (1 point)\n",
        "Basically, there are 5 states, and two actions 'a' and 'b'. Each transition (s,a,s') is noted with its corresponding reward. You are to first familiarize with the framework using its [documentation](http://gym.openai.com/docs/), and then implement the \"$\\epsilon$-greedy Q-learning\" algorithm for the Chain enviroment (called 'NChain-v0') using default parameters. Finally print the $Q^*$ table at convergence. Take $\\gamma=0.95$. \n",
        "\n",
        "Hints:\n",
        "* You can refer to the Q-learning Jupyter notebook shown in class, uploaded on Canvas.\n",
        "* Try some decreasing sequences of $\\epsilon$ (1/t etc.) with small fixed learning rate $\\alpha$.\n",
        "* Latest Gym version (0.23.1) does not have the Chain environment, you need to install any version $\\leq 0.19.0$. You can do so in Google colab by command: \" !pip install gym==version \"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kGV-q2zuCQss"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "# !pip install gym==0.19.0\n",
        "import gym\n",
        "import random\n",
        "random.seed(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PkkQSipCd6A",
        "outputId": "fed0f253-0f31-4200-82aa-4c60d20b9e3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q* Table\n",
            "array([[61.36693482, 60.56255399],\n",
            "       [64.87980726, 61.44537806],\n",
            "       [69.51317913, 62.62931856],\n",
            "       [75.56102813, 64.0094001 ],\n",
            "       [83.61251316, 66.0830927 ]])\n",
            "\n",
            "Optimal V from Q-learning\n",
            "array([61.36693482, 64.87980726, 69.51317913, 75.56102813, 83.61251316])\n"
          ]
        }
      ],
      "source": [
        "# make chain environment\n",
        "env = gym.make(\"NChain-v0\")\n",
        "\n",
        "# set hyperparameters for the model\n",
        "gamma = 0.95 # discount rate\n",
        "alpha = 0.1 # learning rate\n",
        "epsilon = 0.5\n",
        "\n",
        "# initialize the Q table, 5 states and 2 actions\n",
        "# create an empty 5x2 matrix of zeros\n",
        "q = np.zeros([5, 2])\n",
        "\n",
        "# create an empty 5x2 matrix of ones\n",
        "q_old = np.ones([5, 2])\n",
        "\n",
        "iteration = 0\n",
        "\n",
        "while not np.allclose(q_old, q, atol = 0.001):\n",
        "    iteration += 1\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    # reduce the alpha value to make the matrix converge\n",
        "    alpha *= 0.999 \n",
        "    \n",
        "    # make a copy of the q table\n",
        "    q_old = q.copy()\n",
        "    \n",
        "    while not done:\n",
        "        # First we select an action:\n",
        "        if random.uniform(0, 1) < epsilon: # Flip a coin\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q[state,:]) # Exploit learned values\n",
        "\n",
        "        # Then we perform the action and receive the feedback from the environment\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Finally we learn from the experience by updating the Q-value of the selected action\n",
        "        prediction_error = reward + (gamma * np.max(q[new_state, :])) - q[state, action]\n",
        "        q[state, action] += alpha * prediction_error \n",
        "        state = new_state\n",
        "\n",
        "print(\"Q* Table\")\n",
        "pprint(q)\n",
        "\n",
        "print(\"\\nOptimal V from Q-learning\")\n",
        "pprint(np.max(q, axis = 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2jiyDIRCd6A"
      },
      "source": [
        "## Question 4 (2 points)\n",
        "\n",
        "Verify that the optimal $Q^*$ value obtained using Q-learning is same as the optimal value function $V^*$ (for the corresponding optimal action). You would have to first define the MDP corresponding to Chain enviroment.\n",
        "\n",
        "\n",
        "Hint: \n",
        "\n",
        "* \"Define the MDP..\" means to define \"$P_a(s,s')$\" and \"$R_a(s,s')$\" tables for the Chain environment. Its good to know that gym simulator that you use in Q3 takes the correct action with a probability of 0.8 (slip=0.2). This information is useful in defining $P_a(s,s')$ correctly (you can check the nchain.py file by going to the folder specified by 'print(gym.envs.toy_text)' command).\n",
        "\n",
        "* Once the \"p\" and \"r\" tables are defined, you can use your VI implementation from Q2 to compute the \"V*(s)\". Then the following relation must hold: $V^*(s) = max_a Q^*(s)$ for all s. This proves that the optimal solution can be obtained using Q-learning without knowing the underlying environment model (MDP), unlike the VI algorithm, which needs $P_a,R_a$ information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wce0zSVfCd6B",
        "outputId": "bdcb543e-ef3c-4b0a-e2bb-5cfd09761f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pi (Index of Max Reward): 4\n",
            "\n",
            "Optimal V from Value Iteration:\n",
            "[61.35026977428045,\n",
            " 64.86207777428045,\n",
            " 69.48287777428047,\n",
            " 75.56287777428045,\n",
            " 83.56287777428047]\n"
          ]
        }
      ],
      "source": [
        "def value_iteration_2(success_proba, gamma, reward_mtx, v_k_old, v_k):\n",
        "\n",
        "    while not np.allclose(v_k_old, v_k, atol = 0.001):\n",
        "        \n",
        "        # create copy of current v_k\n",
        "        v_k_old = v_k.copy()\n",
        "        \n",
        "        # set current v_k as empty array\n",
        "        v_k = []\n",
        "\n",
        "        for x in range(len(v_k_old)):\n",
        "            # Next state index, either r+1 or 4 if we're at the \"end\"\n",
        "            next_state = min(x + 1, 4)\n",
        "\n",
        "            # Score after transition `a`\n",
        "            score_a = success_proba * (reward_mtx[x][0] + gamma * v_k_old[next_state]) \\\n",
        "                    + (1 - success_proba) * (reward_mtx[x][1] + gamma * v_k_old[0])\n",
        "\n",
        "            # Score after transition `b`\n",
        "            score_b = success_proba * (reward_mtx[x][1] + gamma * v_k_old[0]) \\\n",
        "                    + (1 - success_proba) * (reward_mtx[x][0] + gamma * v_k_old[next_state])\n",
        "\n",
        "            v_k.append(max(score_a, score_b))\n",
        "\n",
        "    # find the index of the max reward\n",
        "    pi = np.argmax(np.array(v_k))\n",
        "\n",
        "    return pi, v_k\n",
        "\n",
        "# set the successfull action probability\n",
        "success_proba = 0.8\n",
        "\n",
        "# set the discount factor (gamma)\n",
        "gamma = 0.95\n",
        "\n",
        "# set initial reward matrix\n",
        "reward_mtx = [\n",
        "    [0, 2],\n",
        "    [0, 2],\n",
        "    [0, 2],\n",
        "    [0, 2],\n",
        "    [10, 2]\n",
        "]\n",
        "\n",
        "# create an empty array of zeros\n",
        "v_k = list(np.zeros((5)))\n",
        "\n",
        "# create an empty array of ones\n",
        "v_k_old = list(np.ones((5)))\n",
        "\n",
        "pi, v_k = value_iteration_2(success_proba, gamma, reward_mtx, v_k_old, v_k)\n",
        "\n",
        "print(f\"Pi (Index of Max Reward): {pi}\")\n",
        "print(\"\\nOptimal V from Value Iteration:\")\n",
        "pprint(v_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSI7lXPsCd6B"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "Now, we check whether Q* is approximately equal to the V* value for each state.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MWvuuUOCd6B",
        "outputId": "39e18902-ce36-48ed-88ee-824d9960dbdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result from Q*: [61.36693482 64.87980726 69.51317913 75.56102813 83.61251316]\n",
            "Result from V*: [61.35026977428045, 64.86207777428045, 69.48287777428047, 75.56287777428045, 83.56287777428047]\n",
            "\n",
            "Average difference between Q* and V*: 0.023, i.e. 0.033%\n",
            "\n",
            "Is Q* and V* approximately equal?\tTrue\n"
          ]
        }
      ],
      "source": [
        "# Average difference between the two models\n",
        "abs_diff = 0\n",
        "# Average value of a cell\n",
        "avg = 0\n",
        "\n",
        "for i in range(len(v_k)):\n",
        "    abs_diff += abs(np.max(q, axis = 1)[i] - v_k[i])\n",
        "    avg += (np.max(q, axis = 1)[i] + v_k[i]) / 2\n",
        "\n",
        "abs_diff /= len(v_k)\n",
        "avg /= len(v_k)\n",
        "diff_perc = abs_diff / avg * 100\n",
        "\n",
        "print(\"Result from Q*:\", np.max(q, axis = 1))\n",
        "print(\"Result from V*:\", v_k)\n",
        "\n",
        "print(f\"\\nAverage difference between Q* and V*: {round(abs_diff, 3)}, i.e. {round(diff_perc, 3)}%\")\n",
        "\n",
        "print(f\"\\nIs Q* and V* approximately equal?\\t{np.allclose(np.max(q, axis = 1), v_k,  atol=0.025)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xafo4E12Cd6C"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "We can see the results are approximately equal, with an average difference of less than <b>0.025%</b>. The difference is probably because we did not iterate through the code till the matrices entirely converged (we checked if the old and new matrices had a difference of less than <b>0.001</b>).\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9j8NrTFCd6C"
      },
      "source": [
        "## Question 5 (3 point)\n",
        "\n",
        "* What was the significance of exploration in RL? How did you control it in the case Q-learning algorithm? (1.5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk37flSICd6C"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "In reinforcement learning, the model is rewarded for executing behaviours in various states, and exploration is crucial as it allows the algorithm to probe its surrounding area and evolve further. Exploitation is the inverse of exploration, and if the model only exploits, it might be reduced to a greedy algorithm. \"Exploration\" entails \"risking\" receiving no immediate benefit in exchange for the possibility of receiving a higher payoff in the future.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15QNzrlTI_-a"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "Let us use the Chain problem above to demonstrate the notion, and the table illustrates the payoff for doing the actions of moving forwards or backwards at each state.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo2IU4ScI94W"
      },
      "source": [
        "|   state       |   forwards    |    backwards    |\n",
        "|---------------|-----------|---------------|\n",
        "|   1           |   0       |   2           |\n",
        "|   2           |   0       |   2           |\n",
        "|   3           |   0       |   2           |\n",
        "|   4           |   0       |   2           |\n",
        "|   5           |   10      |   2           |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQfCOCNTI67D"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "We can see that the maximum reward (10) is obtained by being in state five and moving forward, which surpasses all other potential rewards and is always preferred.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHtYBtS7I5Gx"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "If no exploration is used, the model might never realise that state 5 exists on which the forward action yields a significantly higher reward than the other actions. Reaching state five, however, requires picking actions which do not yield any rewards for some actions. Thus if the model never explores and finds state 5, staying in state one by using action b will seem like the optimal strategy to yield the most immediate results.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ8gHyAPI2fB"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "Exploration is essential because it aids in discovering new states, which may have better rewards. In conclusion, exploitation will always discover a local optimum, whereas a model that explores sufficiently will find the global optimum.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irQdzAioI0Cy"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "In our case, we used the \"$\\epsilon$-greedy\" strategy to control exploration in which the agent both exploits to take advantage of prior knowledge and explores to look for new options. This approach aims to identify a possible way and keep on exploiting it 'greedily'. The agent does random exploration occasionally with probability and takes the optimal action most of the time with probability. \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXN2RDtkIxaO"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "This strategy chooses the action with the highest anticipated reward most of the time. The goal is to find a balance between exploration and exploitation. We opt to explore with a low likelihood of *$\\epsilon$*, i.e. not using what we have learnt thus far. In this situation, the action is chosen randomly, regardless of the action-value estimates. If we do an infinite number of trials, each action is performed an infinite number of times. As a result, the epsilon-greedy action selection policy always finds the best actions. The best action, i.e. the one with the highest Q-value, is chosen. Otherwise, the programme will investigate a random action.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1utEOabCd6C"
      },
      "source": [
        "* Briefly discuss the k-armed bandit problem formulation and it's distinguishing feature as a special case of the reinforcement learning problem formulation. (1.5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kgxGHNACd6C"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "In the k-armed bandit problem, each action has an expected or mean reward. The original form of the k-armed bandit problem, called after a slot machine, contains k levers. Each action choice is analogous to pulling one of the slot machine's levers, and the prizes are the payoffs for winning the jackpot. The aim is to maximise one's winnings by concentrating their actions on the best levers. \n",
        "</span>\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "Another analogy is a news website determining which stories to show to readers. As the website does not have any data on the readers, they do not know all click outcomes. The aim is to select articles that attract the most attention and adequately arrange them to increase interaction. However, there is a lot of content and very few statistics that help identify a precise strategy.\n",
        "</span>\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "The k-armed bandit problem is connected to reinforcement learning because it has only one state but multiple transitions with varying costs. The goal is to discover a \"smarter\" method that does not blindly loop on a low-reward low-risk state. Thus, the distinction between exploration and exploitation is critical in this problem.\n",
        "</span>\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "If one maintains estimates of the action values, then at any time step, at least one action's estimated value is highest. Such actions are called greedy actions, which, when selected, exploit one's current knowledge of the values of the actions. On the other hand, if nongreedy actions are selected, improve the nongreedy action's value estimate, i.e. exploration. Exploitation helps to maximise the predicted value in one step, while exploration helps find a higher total reward in the long run. In reinforcement learning, balancing exploration and exploitation is a distinctive challenge.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb9R6RicCd6D"
      },
      "source": [
        "## Note\n",
        "\n",
        "* Until now, we have described algorithms for when no. of states and actions are finite. In coming weeks, you will be taught how to extend these methods to continous state enviroments like ATARI games.\n",
        "\n",
        "# References\n",
        "Primer/text based on the following references:\n",
        "* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n",
        "* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf\n",
        "* 2.1 An n-Armed Bandit Problem. http://incompleteideas.net/book/2/node2.html."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RLAssignment-LP4-2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.9 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
